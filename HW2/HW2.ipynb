{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John Shamshoian\n",
    "\n",
    "Homework 2\n",
    "\n",
    "## Q1\n",
    "\n",
    "Consider the numerical task of estimating an $n \\times n$ kinship matrix $\\Phi$ from an $n \\times m$ genotype matrix $\\mathbf{X}$. Here $n$ is the number of individuals and $m$ is the number of genetic markers. [Lange et al](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4062304/) derived a method of moment estimator of form\n",
    "$$\n",
    "    \\widehat \\Phi_{ij} = \\frac{e_{ij} - \\sum_{k=1}^m [p_k^2 + (1 - p_k)^2]}{m - \\sum_{k=1}^m [p_k^2 + (1 - p_k)^2]}, \\quad 1 \\le i, j \\le n,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "    e_{ij} &=& \\frac{1}{4} \\sum_{k=1}^m [x_{ik} x_{jk} + (2 - x_{ik})(2 - x_{jk})] \\\\\n",
    "    p_k &=& \\frac {1}{2n} \\sum_{i=1}^n x_{ik}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "0. Write a function that takes a matrix `X` as input and outputs the method of moment estimator\n",
    "    ```julia\n",
    "    function kinship(X::Matrix{Float64})\n",
    "        # your code here\n",
    "        return Î¦\n",
    "    end\n",
    "    ```\n",
    "Make your function as efficient as possible.    \n",
    "\n",
    "0. In a typical genetic data set, $n$ is at order of $10^3 \\sim 10^5$ and $m$ is at order of $10^5 \\sim 10^6$. Benchmark your function using a smaller data set\n",
    "    ```julia\n",
    "    srand(280) # seed\n",
    "    X = rand(0.0:2.0, 1000, 10000)\n",
    "    ```\n",
    "Efficiency (both speed and memory) will be the most important criterion when grading this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "srand(280) # seed\n",
    "X = rand(0.0:2.0, 1000, 10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kinship (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function kinship(X::Matrix{Float64})\n",
    "    \n",
    "    # Get dimensions of the input matrix. \n",
    "    n = size(X, 1)\n",
    "    m = size(X, 2)\n",
    "    \n",
    "    # Create matrix to hold phi. Preallocating memory is a good thing\n",
    "    phi = Matrix{Float64}(n, n)\n",
    "    p = sum(X, 1) / (2.0 * n)\n",
    "    \n",
    "    # Use sumabs2 to quickly compute sum of squared components in a vector\n",
    "    pnorm = sumabs2(p)\n",
    "    pnorm2 = sumabs2(1 - p)\n",
    "    \n",
    "    # Use this to hold row-sums of X\n",
    "    taco = sum(X, 2)\n",
    "    \n",
    "    # Use BLAS to take advantage of symmetry when multiplying matrices. \n",
    "    # Much more efficient than writing X * X'.\n",
    "    # This is an intermediate step in creating the 'e' matrix. \n",
    "    # I don't explicitly create the e matrix because\n",
    "    # that would cost more memory. Overwriting is better in terms of\n",
    "    # memory and speed.\n",
    "    phi = 1.0 / 4.0 * (Base.LinAlg.BLAS.syr!('L', 4.0 * m, ones(n),\n",
    "                Base.LinAlg.BLAS.syrk('L', 'N', 2.0, X)))\n",
    "    \n",
    "    # Overwrite phi to create the lower triangular part of phi\n",
    "    for j = 1:n\n",
    "        for i = j:n\n",
    "            phi[i, j] = (phi[i, j] - 1.0 / 2.0 * (taco[i] + taco[j]) - \n",
    "                pnorm - pnorm2) / (m - pnorm - pnorm2)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Fill in the upper triangular part of phi. Phi is symmetric \n",
    "    # so I can loop this way.\n",
    "    for j = 2:n\n",
    "        for i = 1:j\n",
    "            phi[i, j] = phi[j, i]\n",
    "        end\n",
    "    end\n",
    "    phi\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  23.13 MiB\n",
       "  allocs estimate:  22\n",
       "  --------------\n",
       "  minimum time:     133.339 ms (0.21% GC)\n",
       "  median time:      148.170 ms (0.95% GC)\n",
       "  mean time:        148.076 ms (0.91% GC)\n",
       "  maximum time:     171.237 ms (0.79% GC)\n",
       "  --------------\n",
       "  samples:          34\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark kinship(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My kinship function takes about 150ms on my computer and does little garbage collecting. Dr. Zhou's kinship function took 90ms but his CPU is faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "0. Show the **Sherman-Morrison formula**\n",
    "$$\n",
    "\t(\\mathbf{A} + \\mathbf{u} \\mathbf{u}^T)^{-1} = \\mathbf{A}^{-1} - \\frac{1}{1 + \\mathbf{u}^T \\mathbf{A}^{-1} \\mathbf{u}} \\mathbf{A}^{-1} \\mathbf{u} \\mathbf{u}^T \\mathbf{A}^{-1},\n",
    "$$\n",
    "where $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is nonsingular and $\\mathbf{u} \\in \\mathbb{R}^n$. This formula supplies the inverse of the symmetric, rank-one  perturbation of $\\mathbf{A}$.\n",
    "\n",
    "0. Show the **Woodbury formula**\n",
    "$$\n",
    "\t(\\mathbf{A} + \\mathbf{U} \\mathbf{V}^T)^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{I}_m + \\mathbf{V}^T \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V}^T \\mathbf{A}^{-1},\n",
    "$$\n",
    "where $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is nonsingular, $\\mathbf{U}, \\mathbf{V} \\in \\mathbb{R}^{n \\times m}$, and $\\mathbf{I}_m$ is the $m \\times m$ identity matrix. In many applications $m$ is much smaller than $n$. Woodbury formula generalizes Sherman-Morrison and is valuable because the smaller matrix $\\mathbf{I}_m + \\mathbf{V}^T \\mathbf{A}^{-1} \\mathbf{U}$ is cheaper to invert than the larger matrix $\\mathbf{A} + \\mathbf{U} \\mathbf{V}^T$.\n",
    "\n",
    "0. Show the **binomial inversion formula**\n",
    "$$\n",
    "\t(\\mathbf{A} + \\mathbf{U} \\mathbf{B} \\mathbf{V}^T)^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{B}^{-1} + \\mathbf{V}^T \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V}^T \\mathbf{A}^{-1},\n",
    "$$\n",
    "where $\\mathbf{A}$ and $\\mathbf{B}$ are nonsingular.\n",
    "\n",
    "0. Show the identity\n",
    "$$\n",
    "\t\\text{det}(\\mathbf{A} + \\mathbf{U} \\mathbf{V}^T) = \\text{det}(\\mathbf{A}) \\text{det}(\\mathbf{I}_m + \\mathbf{V}^T \\mathbf{A}^{-1} \\mathbf{U}).\n",
    "$$\n",
    "This formula is useful for evaluating the density of a multivariate normal with covariance matrix $\\mathbf{A} + \\mathbf{U} \\mathbf{V}^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Assume $(\\mathbf{A} + \\mathbf{uu}^{T})$ is invertible. I will show the Sherman-Morrison formula satisfies the properties of an inverse.\n",
    "$$\n",
    "\\begin{align}\n",
    "(\\mathbf{A} + \\mathbf{uu}^{T})\\bigg(\\mathbf{A}^{-1} - \\frac{1}{1+\\mathbf{u}^{T}\\mathbf{A}^{-1}\\mathbf{u}}\\mathbf{A}^{-1}\\mathbf{uu}^{T}\\mathbf{A}^{-1}\\bigg) &=\\mathbf{I} - \\frac{\\mathbf{uu}^{T}\\mathbf{A}^{-1}}{1+\\mathbf{u}^{T}\\mathbf{A}^{-1}\\mathbf{u}} + \\mathbf{uu}^{T}A^{-1}\\mathbf{} -\\frac{\\mathbf{uu}^{T}\\mathbf{A}^{-1}\\mathbf{uu}^{T}\\mathbf{A}^{-1}}{1 + \\mathbf{u}^{T}\\mathbf{A}^{-1}\\mathbf{u}}\\\\\n",
    "&=\\mathbf{I} - \\frac{\\mathbf{uu}^{T}\\mathbf{A}^{-1}}{1+\\mathbf{u}^{T}\\mathbf{A}^{-1}\\mathbf{u}} + \\frac{\\mathbf{uu}^{T}A^{-1}+\\mathbf{u}^{T}\\mathbf{A}^{-1}\\mathbf{u}\\mathbf{uu}^{T}\\mathbf{A}^{-1}}{1+\\mathbf{u}^{T}\\mathbf{A}^{-1}\\mathbf{u}} -\\frac{\\mathbf{uu}^{T}\\mathbf{A}^{-1}\\mathbf{uu}^{T}\\mathbf{A}^{-1}}{1 + \\mathbf{u}^{T}\\mathbf{A}^{-1}\\mathbf{u}}\\\\\n",
    "&=\\mathbf{I} + \\frac{\\mathbf{uu}^{T}\\mathbf{A}^{-1}\\mathbf{uu}^{T}\\mathbf{A}^{-1}}{1+\\mathbf{u}^{T}\\mathbf{A}^{-1}\\mathbf{u}}-\\frac{\\mathbf{uu}^{T}\\mathbf{A}^{-1}\\mathbf{uu}^{T}\\mathbf{A}^{-1}}{1 + \\mathbf{u}^{T}\\mathbf{A}^{-1}\\mathbf{u}}\\\\\n",
    "&=\\mathbf{I}\n",
    "\\end{align}\n",
    "$$\n",
    "This shows the Sherman-Morrison formula is a right-inverse. I need to check it's a left-inverse as well.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bigg(\\mathbf{A}^{-1} - \\frac{1}{1+\\mathbf{u}^{T}\\mathbf{A}^{-1}\\mathbf{u}}\\mathbf{A}^{-1}\\mathbf{uu}^{T}\\mathbf{A}^{-1}\\bigg) (\\mathbf{A} + \\mathbf{uu}^{T})&=\\mathbf{I} + \\mathbf{A}^{-1}\\mathbf{uu}^{T} - \\frac{\\mathbf{A}^{-1}\\mathbf{uu}^{T}}{1+\\mathbf{u}^{T}\\mathbf{A}^{-1}\\mathbf{u}} - \\frac{\\mathbf{A}^{-1}\\mathbf{uu}^{T}\\mathbf{A}^{-1}\\mathbf{uu}^{T}}{1+\\mathbf{u}^{T}\\mathbf{A}^{-1}\\mathbf{u}}\\\\\n",
    "&=\\mathbf{I} + \\frac{\\mathbf{A}^{-1}\\mathbf{uu}^{T}+\\mathbf{u}^{T}\\mathbf{A}^{-1}\\mathbf{u}\\mathbf{A}^{-1}\\mathbf{uu}^{T}-\\mathbf{A}^{-1}\\mathbf{uu}^{T}-\\mathbf{A}^{-1}\\mathbf{uu}^{T}\\mathbf{A}^{-1}\\mathbf{uu}^{T}}{1+\\mathbf{u}^{T}\\mathbf{A}^{-1}\\mathbf{u}}\\\\\n",
    "&= \\mathbf{I}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    " By definition of inverse,  $(\\mathbf{A} + \\mathbf{uu}^{T})^{-1}=\\mathbf{A}^{-1} - \\displaystyle\\frac{1}{1 + \\mathbf{u}^T \\mathbf{A}^{-1} \\mathbf{u}} \\mathbf{A}^{-1} \\mathbf{u} \\mathbf{u}^T \\mathbf{A}^{-1}$\n",
    " \n",
    "2. In a similar manner,\n",
    "$$\n",
    "\\begin{align}\n",
    "(\\mathbf{A} + \\mathbf{UV}^{T})(\\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{I}_m + \\mathbf{V}^T \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V}^T \\mathbf{A}^{-1})&=\\mathbf{I_n} + \\mathbf{UV}^{T}\\mathbf{A}^{-1}-\\mathbf{U}(\\mathbf{I_m}\n",
    "+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})^{-1}\\mathbf{V}^{T}\\mathbf{A}^{-1} - \\mathbf{UV}^{T}\\mathbf{A}^{-1}\\mathbf{U}(\\mathbf{I_m}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})\\mathbf{V}^{T}\\mathbf{A}^{-1}\\\\\n",
    "&=\\mathbf{I_n}+\\mathbf{UV}^{T}\\mathbf{A}^{-1}-(\\mathbf{U}+\\mathbf{UV}^{T}\\mathbf{A}^{-1}\\mathbf{U})(\\mathbf{I_m}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})^{-1}\\mathbf{V}^{T}\\mathbf{A}^{-1}\\\\\n",
    "&=\\mathbf{I_n} + \\mathbf{UV}^{T}\\mathbf{A}^{-1}-\\mathbf{U}(\\mathbf{I_m}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})(\\mathbf{I_m}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})^{-1}\\mathbf{V}^{T}\\mathbf{A}^{-1}\\\\\n",
    "&=\\mathbf{I_n}+\\mathbf{UV}^{T}\\mathbf{A}^{-1}-\\mathbf{U}\\mathbf{V}^{T}\\mathbf{A}^{-1}\\\\\n",
    "&=\\mathbf{I_n}\n",
    "\\end{align}\n",
    "$$\n",
    " \n",
    " This shows the Woodbury formula is a right inverse for $\\mathbf{A} + \\mathbf{UV}^{T}$. To show it's a left inverse:\n",
    "$$\n",
    "\\begin{align}\n",
    "(\\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{I}_m + \\mathbf{V}^T \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V}^T \\mathbf{A}^{-1})(\\mathbf{A} + \\mathbf{UV}^{T})&= \\mathbf{I_n} + \\mathbf{A}^{-1}\\mathbf{UV}^{T}-\\mathbf{A}^{-1}\\mathbf{U}(\\mathbf{I_m}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})^{-1}\\mathbf{V}^{T}-\\mathbf{A}^{-1}\\mathbf{U}(\\mathbf{I_m}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})^{-1}\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{UV}^{T}\\\\\n",
    "&=\\mathbf{I_n}+\\mathbf{A}^{-1}\\mathbf{UV}^{T}-\\mathbf{A}^{-1}\\mathbf{U}(\\mathbf{I_m}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})^{-1}(\\mathbf{V}^{T}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{UV}^{T})\\\\\n",
    "&=\\mathbf{I_n}+\\mathbf{A}^{-1}\\mathbf{UV}^{T}-\\mathbf{A}^{-1}\\mathbf{U}(\\mathbf{I_m}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})^{-1}(\\mathbf{I_m}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})\\mathbf{V}^{T}\\\\\n",
    "&=\\mathbf{I_n}+\\mathbf{A}^{-1}\\mathbf{UV}^{T}-\\mathbf{A}^{-1}\\mathbf{UV}^{T}\\\\\n",
    "&=\\mathbf{I_n}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    " By definition of inverse, $(\\mathbf{A} + \\mathbf{UV}^{T})^{-1}=(\\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{I}_m + \\mathbf{V}^T \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V}^T \\mathbf{A}^{-1})$\n",
    " \n",
    "3. Continuing, \n",
    "$$\n",
    "\\begin{align}\n",
    "(\\mathbf{A}+\\mathbf{UBV}^{T})(\\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{B}^{-1} + \\mathbf{V}^T \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V}^T \\mathbf{A}^{-1})&= \\mathbf{I_n}-\\mathbf{U}(\\mathbf{B}^{-1}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})^{-1}\\mathbf{V}^{T}\\mathbf{A}^{-1}+\\mathbf{UBV}^{T}\\mathbf{A}^{-1}-\\mathbf{UBV}^{T}\\mathbf{A}^{-1}\\mathbf{U}(\\mathbf{B}^{-1}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})^{-1}\\mathbf{V}^{T}\\mathbf{A}^{-1}\\\\\n",
    "&=\\mathbf{I_n}+\\mathbf{UBV}^{T}\\mathbf{A}^{-1}-(\\mathbf{U}+\\mathbf{UBV}^{T}\\mathbf{A}^{-1}\\mathbf{U})(\\mathbf{B}^{-1}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})^{-1}\\mathbf{V}^{T}\\mathbf{A}^{-1}\\\\\n",
    "&=\\mathbf{I_n}+\\mathbf{UBV}^{T}\\mathbf{A}^{-1}-\\mathbf{UB}(\\mathbf{B}^{-1}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})(\\mathbf{B}^{-1}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})^{-1}\\mathbf{V}^{T}\\mathbf{A}^{-1}\\\\\n",
    "&=\\mathbf{I_n}+\\mathbf{UBV}^{T}\\mathbf{A}^{-1}-\\mathbf{UBV}^{T}\\mathbf{A}^{-1}\\\\\n",
    "&=\\mathbf{I_n}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    " Left inverse:\n",
    "$$\n",
    "\\begin{align}\n",
    "(\\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{B}^{-1} + \\mathbf{V}^T \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V}^T \\mathbf{A}^{-1})(\\mathbf{A}+\\mathbf{UBV}^{T})&=\\mathbf{I_n} + \\mathbf{A}^{-1}\\mathbf{UBV}^{T}-\\mathbf{A}^{-1}\\mathbf{U}(\\mathbf{B}^{-1}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})^{-1}-\\mathbf{A}^{-1}\\mathbf{U}(\\mathbf{B}^{-1}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})^{-1}\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{UBV}^{T}\\\\\n",
    "&=\\mathbf{I_n}+\\mathbf{A}^{-1}\\mathbf{UBV}^{T}-\\mathbf{A}^{-1}\\mathbf{U}(\\mathbf{B}^{-1}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})^{-1}(\\mathbf{V}^{T}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{UBV}^{T})\\\\\n",
    "&=\\mathbf{I_n}+\\mathbf{A}^{-1}\\mathbf{UBV}^{T}-\\mathbf{A}^{-1}\\mathbf{U}(\\mathbf{B}^{-1}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})^{-1}(\\mathbf{B}^{-1}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})\\mathbf{BV}^{T}\\\\\n",
    "&=\\mathbf{I_n}+\\mathbf{A}^{-1}\\mathbf{UBV}^{T}-\\mathbf{A}^{-1}\\mathbf{UBV}^{T}\\\\\n",
    "&=\\mathbf{I_n}\n",
    "\\end{align}\n",
    "$$\n",
    " This shows $(\\mathbf{A}+\\mathbf{UBV}^{T})^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{B}^{-1} + \\mathbf{V}^T \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V}^T \\mathbf{A}^{-1}$\n",
    " \n",
    "4. Observate that\n",
    " $$\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\\mathbf{I_n} & \\mathbf{U}\\\\\\mathbf{0_{m\\times n}} & \\mathbf{I_m}+\\mathbf{V}^{T}\\mathbf{U}\\end{pmatrix} = \\begin{pmatrix}\\mathbf{I_n} & \\mathbf{0_{n\\times m}}\\\\\\mathbf{V}^{T} & \\mathbf{I_m}\\end{pmatrix}\\begin{pmatrix}\\mathbf{I_n}+\\mathbf{U}\\mathbf{V}^{T} & \\mathbf{U}\\\\\\mathbf{0_{m\\times n}} & \\mathbf{I_m}\\end{pmatrix}\\begin{pmatrix}\\mathbf{I_n} & \\mathbf{0_{n\\times m}}\\\\-\\mathbf{V}^{T} & \\mathbf{I_m}\\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    " The determinant of a product is the product of determinants. Thus, $\\text{det}(\\mathbf{I_n} + \\mathbf{UV}^{T}) = \\text{det}(\\mathbf{I_m}+\\mathbf{V}^{T}\\mathbf{U})$\n",
    " \n",
    " Therefore\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{det}(\\mathbf{A}+\\mathbf{U}\\mathbf{V}^{T}) &=\\text{det}(\\mathbf{A}+\\mathbf{A}\\mathbf{A}^{-1}\\mathbf{UV}^{T})\\\\\n",
    "&=\\text{det}(\\mathbf{A})\\text{det}(\\mathbf{I_n}+\\mathbf{A}^{-1}\\mathbf{UV}^{T})\\\\\n",
    "&=\\text{det}(\\mathbf{A})\\text{det}(\\mathbf{I_m}+\\mathbf{V}^{T}\\mathbf{A}^{-1}\\mathbf{U})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "Consider a mixed effects model\n",
    "$$\n",
    "\ty_i = \\mathbf{x}_i^T \\beta + \\mathbf{z}_i^T \\gamma + \\epsilon_i, \\quad i=1,\\ldots,n,\n",
    "$$\n",
    "where $\\epsilon_i$ are independent normal errors $N(0,\\sigma_0^2)$, $\\beta \\in \\mathbb{R}^p$ are fixed effects, and $\\gamma \\in \\mathbb{R}^q$ are random effects assumed to be $N(\\mathbf{0}_q, \\sigma_1^2 \\mathbf{I}_q$) independent of $\\epsilon_i$. \n",
    "\n",
    "0. Show that \n",
    "$$\n",
    "    \\mathbf{y} \\sim N \\left( \\mathbf{X} \\beta, \\sigma_0^2 \\mathbf{I}_n + \\sigma_1^2 \\mathbf{Z} \\mathbf{Z}^T \\right),\n",
    "$$\n",
    "where $\\mathbf{y} \\in \\mathbb{R}^n$, $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$, and $\\mathbf{Z} \\in \\mathbb{R}^{n \\times q}$. \n",
    "\n",
    "0. Write a function, with interface \n",
    "    ```julia\n",
    "    logpdf_mvn(y, Z, Ï0, Ï1),\n",
    "    ```\n",
    "that evaluates the log-density of a multivariate normal with mean $\\mathbf{0}$ and covariance $\\sigma_0^2 \\mathbf{I} + \\sigma_1^2 \\mathbf{Z} \\mathbf{Z}^T$ at $\\mathbf{y}$. Make your code efficient in the $n \\gg q$ case. \n",
    "\n",
    "0. Compare your result (both accuracy and timing) to the [Distributions.jl](http://distributionsjl.readthedocs.io/en/latest/multivariate.html#multivariate-normal-distribution) package using following data.  \n",
    "    ```julia\n",
    "    using BenchmarkTools, Distributions\n",
    "\n",
    "    srand(280)\n",
    "    n, q = 2000, 10\n",
    "    Z = randn(n, q)\n",
    "    Ï0, Ï1 = 0.5, 2.0\n",
    "    Î£ = Ï1^2 * Z * Z.' + Ï0^2 * I\n",
    "    mvn = MvNormal(Î£) # MVN(0, Î£)\n",
    "    y = rand(mvn) # generate one instance from MNV(0, Î£)\n",
    "\n",
    "    # check you answer matches that from Distributions.jl\n",
    "    @show logpdf_mvn(y, Z, Ï0, Ï1)\n",
    "    @show logpdf(mvn, y)\n",
    "\n",
    "    # benchmark\n",
    "    @benchmark logpdf_mvn(y, Z, Ï0, Ï1)\n",
    "    @benchmark logpdf(mvn, y)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. From the problem formulation, $\\mathbf{Z}\\gamma \\sim N_n(0, \\mathbf{Z}\\sigma^2_1\\mathbf{Z}^{T})$ and $\\varepsilon \\sim N_n(0,\\sigma^2_0\\mathbf{I_n})$ with $\\mathbf{Z}\\gamma$ independent of $\\varepsilon$\n",
    "\n",
    " Let $\\tilde{\\varepsilon} = \\mathbf{Z}\\gamma + \\varepsilon$. This is the sum of two independent multivariate normal variables, so $\\tilde{\\varepsilon} \\sim N_n(0, \\sigma^2_0\\mathbf{I_n} + \\sigma^2_1\\mathbf{Z}\\mathbf{Z}^{T})$\n",
    " \n",
    " Therefore $\\mathbf{y} = \\mathbf{X}\\beta + \\tilde{\\varepsilon} \\sim N(\\mathbf{X}\\beta, \\sigma^2_0\\mathbf{I_n} + \\sigma^2_1\\mathbf{Z}\\mathbf{Z}^{T})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Question 3, part 2\n",
    "\n",
    "using BenchmarkTools, Distributions\n",
    "\n",
    "srand(280)\n",
    "n, q = 2000, 10\n",
    "Z = randn(n, q)\n",
    "Ï0, Ï1 = 0.5, 2.0\n",
    "Î£ = Ï1^2 * Z * Z.' + Ï0^2 * I\n",
    "mvn = MvNormal(Î£) # MVN(0, Î£)\n",
    "y = rand(mvn); # generate one instance from MNV(0, Î£)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition logpdf_mvn(Array{"
     ]
    },
    {
     "data": {
      "text/plain": [
       "logpdf_mvn (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Float64, 1}, Array{Float64, 2}, Float64, Float64) in module Main at In[11]:2 overwritten at In[14]:4.\n"
     ]
    }
   ],
   "source": [
    "function logpdf_mvn(y::Array{Float64,1}, Z::Array{Float64,2},\n",
    "        Ï0::Float64, Ï1::Float64)\n",
    "    \n",
    "    k = length(y)\n",
    "    \n",
    "    # Using matrix determinant lemma\n",
    "    decomp1 = (4000 * log(Ï0) + logdet(I + Ï1^2 / Ï0^2 * Z' * Z))\n",
    "    \n",
    "    # Using Woodbury formula\n",
    "    Î£chol = cholfact(I + Ï1^2 / Ï0^2 * Z' * Z)\n",
    "    decomp2 = (1/Ï0^2 * sumabs2(y) - Ï1^2 / Ï0^4 * sumabs2(Î£chol[:L] \\ Z' * y))\n",
    "    \n",
    "    # Multivariate normal pdf\n",
    "    - (k//2) * log(2Ï) - (1//2) * decomp1 - (1//2) * decomp2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logpdf_mvn(y,Z,Ï0,Ï1) = -1571.5736734653365\n",
      "logpdf(mvn,y) = -1571.5736734654183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1571.5736734654183"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show logpdf_mvn(y, Z, Ï0, Ï1)\n",
    "@show logpdf(mvn, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  944.81 KiB\n",
       "  allocs estimate:  36\n",
       "  --------------\n",
       "  minimum time:     253.156 Î¼s (0.00% GC)\n",
       "  median time:      371.959 Î¼s (0.00% GC)\n",
       "  mean time:        454.665 Î¼s (15.51% GC)\n",
       "  maximum time:     4.218 ms (45.44% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark logpdf_mvn(y, Z, Ï0, Ï1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  15.78 KiB\n",
       "  allocs estimate:  3\n",
       "  --------------\n",
       "  minimum time:     5.839 ms (0.00% GC)\n",
       "  median time:      6.001 ms (0.00% GC)\n",
       "  mean time:        6.042 ms (0.00% GC)\n",
       "  maximum time:     7.621 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          822\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark logpdf(mvn, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3.3\n",
    "\n",
    "I exploited the special structure of the covariance matrix by using the matrix determinant lemma and Woodbury formula. The timing of my function is 14 times faster on average than the <code>logpdf</code> function. However, my function requires more overhead because I'm not passing a simple multivariate normal density to be evaluated as in the Distributions package. The evaluated densites are equal up to the ninth decimal point. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.1",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
