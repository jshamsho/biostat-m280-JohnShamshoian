{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biostatistics M280 HW 5\n",
    "\n",
    "John Shamshoian\n",
    "\n",
    "## Q1\n",
    "\n",
    "The log-likelihood of the dirichlet-multinomial distribution is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\alpha\\,|\\,\\mathbf{x}) &= \\sum_{i=1}^{n}\\log\\bigg[\\binom{|\\mathbf{x_i}|}{\\mathbf{x_i}}\\bigg] + \\sum_{i=1}^{n}\\sum_{j=1}^{d}\\bigg[\\log\\Gamma(x_{ij}+\\alpha_j)-\\log\\Gamma(\\alpha_j)\\bigg] - \\sum_{i=1}^{n}\\bigg[\\log\\Gamma(|\\mathbf{x_i}|+|\\alpha|)-\\log\\Gamma(|\\alpha|)\\bigg]\\\\\\\\\n",
    "&=\\sum_{i=1}^{n}\\log\\bigg[\\binom{|\\mathbf{x_i}|}{\\mathbf{x_i}}\\bigg] + \\sum_{i=1}^{n}\\sum_{j=1}^{d}\\bigg[\\log\\frac{\\Gamma(x_{ij}+\\alpha_j)}{\\Gamma(\\alpha_j)}\\bigg] - \\sum_{i=1}^{n}\\bigg[\\log\\frac{\\Gamma(|\\mathbf{x_i}|+|\\alpha|)}{\\Gamma(|\\alpha|)}\\bigg]\\\\\\\\\n",
    "&=\\sum_{i=1}^{n}\\log\\bigg[\\binom{|\\mathbf{x_i}|}{\\mathbf{x_i}}\\bigg] + \\sum_{i=1}^{n}\\sum_{j=1}^{d}\\bigg[\\log\\big((\\alpha_{j})(\\alpha_{j}+1)\\cdots(\\alpha_j+x_{ij}-1)\\big)\\bigg] - \\sum_{i=1}^{n}\\bigg[\\log\\big(|\\alpha|)(|\\alpha|+1)\\cdots(|\\alpha|+|\\mathbf{x}|_i-1)\\big)\\bigg]\\\\\\\\\n",
    "&=\\sum_{i=1}^{n}\\log\\bigg[\\binom{|\\mathbf{x_i}|}{\\mathbf{x_i}}\\bigg] + \\sum_{i=1}^{n}\\sum_{j=1}^{d}\\sum_{k=0}^{x_{ij}-1}\\log(\\alpha_j+k) - \\sum_{i=1}^{n}\\sum_{k=0}^{|\\mathbf{x}|_i-1}\\log(|\\alpha|+k)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "The density of the dirichlet distribution is\n",
    "\n",
    "$$\\int_{\\Delta_d}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p} = 1$$\n",
    "\n",
    "Taking derivatives with respect to $\\alpha_k$ on both sides yields\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\frac{d}{d\\alpha_k}\\int_{\\Delta_d}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p} = \\frac{d}{d\\alpha_k}1\\\\\\\\\n",
    "&\\int_{\\Delta_d}\\frac{d}{d\\alpha_k}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}=0\\\\\\\\\n",
    "&\\int_{\\Delta_d}\\bigg(\\frac{\\Gamma'(|\\alpha|)\\prod_{j=1}^{d}\\Gamma(\\alpha_j)-\\Gamma(|\\alpha|)\\Gamma'(\\alpha_k)\\prod_{j\\neq k}^{d}\\Gamma(\\alpha_j)}{\\prod_{j=1}^{d}\\Gamma(\\alpha_j)^{2}}\\bigg)\\prod_{j=1}^d p_j^{\\alpha_j-1}+\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^{d}\\Gamma(\\alpha_j)}\\log(p_k)\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}=0\\\\\\\\\n",
    "&\\rightarrow \\mathbb{E}\\big[\\log(p_k)\\big]=\\int_{\\Delta_d}\\bigg(\\frac{\\Gamma(|\\alpha|)\\Gamma'(\\alpha_k)\\prod_{j\\neq k}^{d}\\Gamma(\\alpha_j)-\\Gamma'(|\\alpha|)\\prod_{j=1}^{d}\\Gamma(\\alpha_j)}{\\prod_{j=1}^{d}\\Gamma(\\alpha_j)^{2}}\\bigg)\\prod_{j=1}^d p_j^{\\alpha_j-1}d\\mathbf{p}\\\\\\\\\n",
    "&\\rightarrow \\mathbb{E}\\big[\\log(p_k)\\big]=\\frac{\\Gamma'(\\alpha_k)}{\\Gamma(\\alpha_k)} - \\frac{\\Gamma'(|\\alpha|)}{\\Gamma(|\\alpha|)}\\\\\\\\\n",
    "&\\rightarrow \\mathbb{E}\\big[\\log(p_k)\\big]=\\Psi(\\alpha_k)-\\Psi(|\\alpha|)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "Treat $p_1,\\ldots,p_d$ as missing data. The first step is to find the conditional distribution $f(\\mathbf{p}\\,|\\,\\mathbf{x}_i, \\alpha)$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(\\mathbf{p}\\,|\\,\\mathbf{x}_i,\\alpha)\\propto f(\\mathbf{x}_i,\\mathbf{p},\\alpha) \\propto f(\\mathbf{x}_i\\,|\\,\\mathbf{p})\\cdot f(\\mathbf{p}\\,|\\,\\alpha)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now, $f(\\mathbf{x}_i\\,|\\,\\mathbf{p})$ is multinomial with parameter $p$. $f(\\mathbf{p}\\,|\\,\\alpha)$ is dirichlet with parameter $\\alpha$. Multiplying these two densities together yields\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(\\mathbf{x}_i,\\mathbf{p},\\alpha) &\\propto f(\\mathbf{x}_i\\,|\\,\\mathbf{p})\\cdot f(\\mathbf{p}\\,|\\,\\alpha)\\\\\\\\\n",
    "&\\propto \\prod_{j=1}^{d}p_j^{x_{ij}}\\prod_{j=1}^{d}p_j^{\\alpha_j-1}\\\\\\\\\n",
    "&\\propto \\prod_{j=1}^{d}p_j^{x_{ij}+\\alpha_j-1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Hence, $f(\\mathbf{p}\\,|\\,\\mathbf{x}_i,,\\alpha)$ is a dirichlet distribution with parameters $x_{ij}+\\alpha_j$. \n",
    "\n",
    "Since $\\alpha$ is a fixed unknown parameter, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(\\mathbf{p},\\mathbf{x}_i\\,|\\,\\alpha)&= f(\\mathbf{x}_i\\,|\\,\\mathbf{p})\\cdot f(\\mathbf{p},\\alpha)\\\\\\\\\n",
    "&=\\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{x_{ij} + \\alpha_j-1}\n",
    "\\end{aligned}\n",
    "$$Following the EM algorithm recipe, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_i(\\alpha\\,|\\,\\alpha^{(t)}) &= \\mathbb{E}_{\\mathbf{p}\\,|\\,\\mathbf{x},\\alpha^{(t)},}\\big[\\log(f(\\mathbf{p},\\mathbf{x}_i\\,|\\,\\alpha))\\big]\\\\\\\\\n",
    "&=\\int_{\\Delta_d}\\frac{\\Gamma(|\\mathbf{x}_i+|\\alpha^{(t)}|)}{\\prod_{j=1}^{d}\\Gamma(x_{ij}+\\alpha_j^{(t)})}\\prod_{j=1}^{d}p_{j}^{x_{ij}+\\alpha_j^{(t)}-1} \\log(f(\\mathbf{p},\\mathbf{x}_i\\,|\\,\\alpha))d\\mathbf{p}\\\\\\\\\n",
    "&=\\int_{\\Delta_d}\\frac{\\Gamma(|\\mathbf{x}_i+|\\alpha^{(t)}|)}{\\prod_{j=1}^{d}\\Gamma(x_{ij}+\\alpha_j^{(t)})}\\prod_{j=1}^{d}p_{j}^{x_{ij}+\\alpha_j^{(t)}-1}\\bigg[\\log\\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\log(\\Gamma(|\\alpha|))-\\sum_{j=1}^{d}\\log(\\Gamma(\\alpha_j))+\\sum_{j=1}^{d}(x_{ij}+\\alpha_j-1)\\log(p_j)\\bigg]d\\mathbf{p}\\\\\\\\\n",
    "&=\\mathbb{E}\\big[\\log\\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\log(\\Gamma(|\\alpha|))-\\sum_{j=1}^{d}\\log(\\Gamma(\\alpha_j))+\\sum_{j=1}^{d}(x_{ij}+\\alpha_j-1)\\log(p_j)\\big]\\\\\\\\\n",
    "&=\\log\\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}+\\log(\\Gamma(|\\alpha|))-\\sum_{j=1}^{d}\\log(\\Gamma(\\alpha_j))+\\sum_{j=1}^{d}(x_{ij}+\\alpha_j-1)\\big(\\Psi(x_{ij}+\\alpha_j^{(t)})-\\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Summing over all $\\mathbf{x}_i$, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(\\alpha|\\alpha^{(t)}) &= \\sum_{i=1}^{n}\\log\\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}+n\\log(\\Gamma(|\\alpha|))-n\\sum_{j=1}^{d}\\log(\\Gamma(\\alpha_j))+\\sum_{i=1}^{n}\\sum_{j=1}^{d}(x_{ij}+\\alpha_j-1)\\big(\\Psi(x_{ij}+\\alpha_j^{(t)})-\\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\big)\\\\\\\\\n",
    "&= c^{(t)} +n\\log(\\Gamma(|\\alpha|)) -n\\sum_{j=1}^{d}\\log\\Gamma(\\alpha_j)) + \\sum_{i=1}^{n}\\sum_{j=1}^{d}\\alpha_j\\big(\\Psi(x_{ij}+\\alpha_j^{(t)})-\\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The $\\alpha$ terms are not separable because of the $\\log(\\Gamma(|\\alpha|))$ term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "Jensen's inequality says $f(tx_1+(1-t)x_2) \\geq tf(x_1) + (1-t)f(x_2)$ for any concave function $f$.\n",
    "\n",
    "Therefore $$\n",
    "\\begin{aligned}\n",
    "\\log(\\alpha_j + k) &= \\log\\Bigg(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}\\frac{\\alpha_j^{(t)}+k}{\\alpha_j^{(t)}}\\alpha_j + \\frac{k}{\\alpha_j^{(t)}+k}\\frac{\\alpha_j^{(t)}+k}{k}k\\Bigg)\\\\\\\\\n",
    "&\\geq \\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}\\log\\bigg(\\frac{\\alpha_j^{(t)}+k}{\\alpha_j^{(t)}}\\alpha_j\\bigg) + c^{(t)}\\\\\\\\\n",
    "&= \\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}\\log(\\alpha_j) + c^{(t)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Supporting hyperplane inequality says $h(y) \\geq h(x) +\\nabla h(x)^{T}(y-x)$ for any convex function $h$.\n",
    "\n",
    "Therefore $-\\log(|\\alpha|+k) \\geq -\\log(|\\alpha^{(t)}|+k) - \\displaystyle\\frac{|\\alpha|-|\\alpha^{(t)}|}{|\\alpha^{(t)}|+k} = c^{(t)}-\\frac{|\\alpha|}{|\\alpha^{(t)}|+k}$\n",
    "\n",
    "Manipulating the triple sum in the log-likelihood, I have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_{i=1}^{n}\\sum_{j=1}^{d}\\sum_{k=0}^{x_{ij}-1}\\log(\\alpha_j+k) &= \\sum_{i=1}^{n}\\sum_{j=1}^{d}\\sum_{k=0}^{max_i x_{ij}-1}\\log(\\alpha_j+k)\\cdot 1_{\\{x_{ij}-1\\geq k\\}}\\\\\\\\\n",
    "&=\\sum_{j=1}^{d}\\sum_{k=0}^{max_i x_{ij}-1}s_{jk}\\log(\\alpha_j + k)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $s_{jk} = \\sum_{i=1}^{n}1_{\\{x_{ij}\\geq k + 1\\}}$\n",
    "\n",
    "The double sum in the log-likelihood can also be manipulated.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_{i=1}^{n}\\sum_{k=0}^{|\\mathbf{x}_i|-1}\\log(|\\alpha| + k) &= \\sum_{i=1}^{n}\\sum_{k=0}^{max_i|\\mathbf{x}_i|-1}\\log(|\\alpha| + k)\\cdot 1_{\\{|\\mathbf{x}_i|-1 \\geq k\\}}\\\\\\\\\n",
    "&=\\sum_{k=0}^{max_i|\\mathbf{x}_i|-1}\\log(|\\alpha| + k)r_k\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $r_k = \\sum_{i=1}^{n}1_{\\{|\\mathbf{x}_i|-1 \\geq k\\}}$\n",
    "\n",
    "The advantage of $s_{jk}$ is it translates the problem from a triple sum to effectively a double sum. It's only a double sum because $s_{jk}$ can be pre-computed. Similar logic for $r_k$.\n",
    "\n",
    "The $g$ function is then\n",
    "\n",
    "$$\n",
    "g(\\alpha\\,|\\,\\alpha^{(t)}) = \\sum_{j=1}^{d}\\sum_{k=0}^{max_i x_{ij}-1}s_{jk}\\frac{\\alpha_{j}}{\\alpha_j^{(t)}+k}\\log(\\alpha_j) - \\sum_{k=0}^{max_i |\\mathbf{x}_i|-1}\\frac{r_k}{|\\alpha^{(t)}|+k} + c^{(t)}\n",
    "$$\n",
    "\n",
    "Taking derivatives with respect to each $\\alpha_j$ and setting equal to zero yields solution\n",
    "\n",
    "$$\n",
    "\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{max_i x_{ij}-1}\\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{max_i |\\mathbf{x}_i|-1}\\frac{r_k}{|\\alpha^{(t)}|+k}}\\alpha_j^{(t)}\n",
    "$$\n",
    "\n",
    "Each term in MM updates are positive. Then as long as $\\alpha^{(0)} > 0$, MM updates are strictly positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "Before I write the MM algorithm, I will read in functions that we used in homework 4. I use these functions within my MM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_logpdf(x, α)\n",
    "\n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at data point `x`.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(x::AbstractVector, α::Vector)\n",
    "    T = promote_type(eltype(x), eltype(α))\n",
    "    xs = sum(x)\n",
    "    αs = sum(α)\n",
    "    if xs == 0 && αs == 0\n",
    "        return zero(T)\n",
    "    elseif xs > 0 && αs == 0\n",
    "        return convert(T, -Inf)\n",
    "    else\n",
    "        l = lfact(xs) - lgamma(xs + αs) + lgamma(αs)\n",
    "    end\n",
    "    for i in eachindex(x)\n",
    "        if α[i] == 0 && x[i] > 0\n",
    "            return convert(T, -Inf)\n",
    "        elseif α[i] > 0\n",
    "            l += - lfact(x[i]) + lgamma(x[i] + α[i]) - lgamma(α[i])\n",
    "        end\n",
    "    end\n",
    "    return l\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at a sample `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::AbstractMatrix, α::Vector)\n",
    "    l  = zero(promote_type(eltype(X), eltype(α)))\n",
    "    for j in 1:size(X, 2)\n",
    "        l += dirmult_logpdf(view(X, :, j), α)\n",
    "    end\n",
    "    return l\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindata = readcsv(\"E:/Classes/BiostatisticsM280/\" *\n",
    "    \"Submissions/biostat-m280-JohnShamshoian/HW4/optdigits.tra\")\n",
    "count = Int64.(traindata[:, 1:64])';\n",
    "digit = Int64.(traindata[:, end]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_score! (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_score(X, α)\n",
    "\n",
    "Evaluate the score (gradient) of Dirichlet-multinomial log-likelihood at `α`.\n",
    "Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_score(X::AbstractMatrix, α::Vector)\n",
    "    T = promote_type(eltype(X), eltype(α))\n",
    "    ∇ = zeros(T, length(α))\n",
    "    return dirmult_score!(∇, X, α)\n",
    "end\n",
    "\n",
    "function dirmult_score!(∇::Vector, X::AbstractMatrix, α::Vector)\n",
    "    fill!(∇, zero(eltype(∇)))\n",
    "    for j in 1:size(X, 2)\n",
    "        dirmult_score!(∇, view(X, :, j), α)\n",
    "    end\n",
    "    return ∇\n",
    "end\n",
    "\n",
    "function dirmult_score!(\n",
    "        ∇::Vector, \n",
    "        x::AbstractVector, \n",
    "        α::Vector\n",
    "    )\n",
    "    \n",
    "    T = promote_type(eltype(x), eltype(α))\n",
    "    xs = zero(eltype(x))\n",
    "    αs = zero(eltype(α))\n",
    "    for i in eachindex(x)\n",
    "        xs += x[i]\n",
    "        αs += α[i]\n",
    "        if α[i] == 0 && x[i] > 0\n",
    "            ∇[i] += Inf\n",
    "        elseif α[i] > 0\n",
    "            ∇[i] += digamma(x[i] + α[i]) - digamma(α[i])\n",
    "        end\n",
    "    end\n",
    "    if αs > 0\n",
    "        c = digamma(xs + αs) - digamma(αs)\n",
    "        for i in eachindex(x)\n",
    "            ∇[i] -= c\n",
    "        end\n",
    "    end\n",
    "    return ∇\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_obsinfo! (generic function with 2 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_obsinfo(X, α)\n",
    "\n",
    "Evaluate the observed information matrix of Dirichlet-multinomial log-likelihood\n",
    "at `α`. Each column of `X` is one data point. Return vector `d` and constant `c`.\n",
    "The observed information matrix equals `Diagonal(d) - c`.\n",
    "\"\"\"\n",
    "function dirmult_obsinfo(X::AbstractMatrix, α::Vector)\n",
    "    T = promote_type(eltype(X), eltype(α))\n",
    "    d = zeros(T, length(α))\n",
    "    return dirmult_obsinfo!(d, X, α)\n",
    "end\n",
    "\n",
    "function dirmult_obsinfo!(d::Vector, X::AbstractMatrix, α::Vector)\n",
    "    T = promote_type(eltype(X), eltype(α))\n",
    "    c = zero(T)\n",
    "    fill!(d, zero(eltype(d)))\n",
    "    for j in 1:size(X, 2)\n",
    "        c += dirmult_obsinfo!(d, view(X, :, j), α)\n",
    "    end\n",
    "    return c, d\n",
    "end\n",
    "\n",
    "function dirmult_obsinfo!(d::Vector, x::AbstractVector, α::Vector)\n",
    "    T  = promote_type(eltype(x), eltype(α))\n",
    "    xs = zero(eltype(x))\n",
    "    αs = zero(eltype(α))\n",
    "    for i in eachindex(x)\n",
    "        xs += x[i]\n",
    "        αs += α[i]\n",
    "        if α[i] == 0 && x[i] > 0\n",
    "            d[i] += T(Inf)\n",
    "        elseif α[i] > 0\n",
    "            d[i] += T(trigamma(α[i]) - trigamma(x[i] + α[i]))\n",
    "        end\n",
    "    end\n",
    "    if αs == 0 && xs > 0\n",
    "        c = T(Inf)\n",
    "    elseif αs == 0 && xs == 0\n",
    "        c = zero(T)\n",
    "    elseif αs > 0\n",
    "        c = T(trigamma(αs) - trigamma(xs + αs))\n",
    "    end\n",
    "    return c\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_mom"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_mom(X)\n",
    "\n",
    "Find the method of moment estimator of Dirichlet-multinomial distribution.\n",
    "Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_mom{T <: Real}(X::Matrix{T})\n",
    "    d, n   = size(X)\n",
    "    α      = zeros(Float64, d)\n",
    "    pi1    = zeros(Float64, d)\n",
    "    pi2    = zeros(Float64, d)\n",
    "    total  = zero(T)\n",
    "    for j in 1:n\n",
    "        colsum = zero(T)\n",
    "        for i in 1:d\n",
    "            α[i]   += X[i, j] # accumulate row sum\n",
    "            colsum += X[i, j]\n",
    "        end\n",
    "        total += colsum\n",
    "        if colsum > 0\n",
    "            for i in 1:d\n",
    "                pi      = X[i, j] / colsum\n",
    "                pi1[i] += pi   # accumulate pi\n",
    "                pi2[i] += pi^2 # accumulate pi^2\n",
    "            end  \n",
    "        end\n",
    "    end\n",
    "    # estiamte ρ and p\n",
    "    ρ = zero(Float64)\n",
    "    for i in 1:d\n",
    "        if pi1[i] > 0\n",
    "            ρ += pi2[i] / pi1[i]\n",
    "        end\n",
    "        α[i] /= total\n",
    "    end\n",
    "    αsum = max((d - ρ) / (ρ - 1), 1e-6)\n",
    "    for i in 1:d\n",
    "        α[i] *= αsum\n",
    "    end\n",
    "    return α\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_newton"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_newton(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using Newton's method.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `d`-by-`n` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: starting point. Default is method of moment estimator.  \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "* `logl`: the log-likelihood at MLE.   \n",
    "* `niter`: the number of iterations performed.\n",
    "# `α`: the MLE.\n",
    "* `∇`: the gradient at MLE. \n",
    "* `obsinfo`: the observed information matrix at MLE. \n",
    "\"\"\"\n",
    "function dirmult_newton(\n",
    "        X::AbstractMatrix; \n",
    "        α0::Vector{Float64} = dirmult_mom(X), \n",
    "        maxiters::Int = 100, \n",
    "        tolfun::Float64 = 1e-6,\n",
    "        verbose::Bool = false\n",
    "    )\n",
    "    \n",
    "    T = promote_type(eltype(X), eltype(α0))\n",
    "    # only consider rows and columns with sum >0\n",
    "    rowind = find(sum(X, 2)) # rows with row sums > 0\n",
    "    colind = find(sum(X, 1)) # cols with col sums > 0\n",
    "    Xwork  = @view X[rowind, colind]\n",
    "    αwork  = α0[rowind]\n",
    "    \n",
    "    # pre-allocate intermediate variables\n",
    "    ∇            = similar(αwork)\n",
    "    obsinfo_dinv = similar(αwork)\n",
    "    newtondir    = similar(αwork)\n",
    "    αnew         = similar(αwork)\n",
    "    loglnew      = zero(T)\n",
    "    \n",
    "    # initial log-L\n",
    "    logliter = dirmult_logpdf(Xwork, αwork)\n",
    "    if verbose\n",
    "        println(\"iteration 0, logl = \", logliter)\n",
    "    end\n",
    "    \n",
    "    # Newton loop\n",
    "    niter = 0\n",
    "    for iter in 1:maxiters\n",
    "        # evaluate gradient (score)\n",
    "        dirmult_score!(∇, Xwork, αwork)\n",
    "        \n",
    "        # approximated observed information matrix\n",
    "        obsinfo_cinv, = dirmult_obsinfo!(obsinfo_dinv, Xwork, αwork)\n",
    "        obsinfo_cinv  = 1 / obsinfo_cinv\n",
    "        map!(x -> 1 / x, obsinfo_dinv)\n",
    "        # shrink c if necessary to make obs. inf. pos def\n",
    "        tmp = sum(obsinfo_dinv)\n",
    "        if obsinfo_cinv ≤ tmp\n",
    "            if verbose; println(\"shrink c\"); end\n",
    "            obsinfo_cinv = 1.05 * tmp\n",
    "        end\n",
    "        \n",
    "        # compute Newton's direction\n",
    "        newtondir .= obsinfo_dinv .* ∇\n",
    "        newtondir .+= (sum(newtondir) / (obsinfo_cinv - tmp)) .* obsinfo_dinv\n",
    "        \n",
    "        # make sure Newton iterate always land within boundary\n",
    "        stepsize = one(T)\n",
    "        for i in eachindex(αwork)\n",
    "            if newtondir[i] < 0\n",
    "                stepsize = min(- αwork[i] / newtondir[i] * 0.95, stepsize)\n",
    "            end\n",
    "        end\n",
    "        # line search loop\n",
    "        for lsiter in 1:10\n",
    "            αnew .= αwork .+ stepsize .* newtondir\n",
    "            loglnew = dirmult_logpdf(Xwork, αnew)\n",
    "            if loglnew > logliter\n",
    "                break\n",
    "            elseif lsiter == 10\n",
    "                println(\"line search failed\")\n",
    "            else\n",
    "                if verbose; println(\"step halving\"); end\n",
    "                stepsize /= 2\n",
    "            end\n",
    "        end\n",
    "        copy!(αwork, αnew)\n",
    "        loglold  = logliter\n",
    "        logliter = loglnew\n",
    "        \n",
    "        # print iterate log-L if requested\n",
    "        if verbose\n",
    "            println(\"iteration \", iter, \", logl = \", logliter)\n",
    "        end\n",
    "        \n",
    "        # check convergence criterion\n",
    "        if abs(logliter - loglold) < tolfun * (abs(loglold) + 1)\n",
    "            niter = iter\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # compute logl, gradient, Hessian from final iterate\n",
    "    αfinal = zeros(eltype(α0), length(α0))\n",
    "    αfinal[rowind] = αwork\n",
    "    ∇final = zeros(eltype(α0), length(α0))\n",
    "    ∇final[rowind] = dirmult_score(Xwork, αwork)\n",
    "    obsinfo = zeros(eltype(α0), length(α0), length(α0))\n",
    "    obsinfo_c, obsinfo_d = dirmult_obsinfo!(obsinfo_dinv, Xwork, αwork)\n",
    "    obsinfo[rowind, rowind] = diagm(obsinfo_d) - obsinfo_c\n",
    "    \n",
    "    # output\n",
    "    return logliter, niter, αfinal, ∇final, obsinfo\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_mm"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_mm(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using MM algorithm.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `d`-by-`n` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: starting point. \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "# Output\n",
    "* `logl`: the log-likelihood at MLE.   \n",
    "* `niter`: the number of iterations performed.\n",
    "# `α`: the MLE.\n",
    "* `∇`: the gradient at MLE. \n",
    "* `obsinfo`: the observed information matrix at MLE. \n",
    "\"\"\"\n",
    "function dirmult_mm(\n",
    "        X::AbstractMatrix; \n",
    "        α0::Vector = dirmult_mom(X), \n",
    "        maxiters::Int = 100, \n",
    "        tolfun = 1e-6,\n",
    "    )\n",
    "    \n",
    "    T = promote_type(eltype(X), eltype(α0))\n",
    "    # only consider rows and columns with sum >0\n",
    "    rowind = find(sum(X, 2)) # rows with row sums > 0\n",
    "    colind = find(sum(X, 1)) # cols with col sums > 0\n",
    "    Xwork  = @view X[rowind, colind]\n",
    "    αwork  = α0[rowind]\n",
    "    \n",
    "    # pre-allocate intermediate variables\n",
    "    ∇            = similar(αwork)\n",
    "    obsinfo_dinv = similar(αwork)\n",
    "    newtondir    = similar(αwork)\n",
    "    loglnew      = zero(T)    \n",
    "    \n",
    "    # Create S matrix\n",
    "    maxS = Int64(maximum(Xwork)) - 1\n",
    "    S = zeros(T, length(αwork), maxS + 1)\n",
    "    for k in 0:maxS\n",
    "        for j in 1:length(αwork)\n",
    "            S[j, k + 1] = sum(Xwork[j, :] .>= k + 1)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Create R vector\n",
    "    maxR = Int64(maximum(sum(Xwork, 1)) - 1)\n",
    "    R = zeros(T, maxR + 1)\n",
    "    for k in 0:maxR\n",
    "        R[k + 1] = sum(sum(Xwork, 1) .>= k + 1)\n",
    "    end\n",
    "    \n",
    "    niter = 0\n",
    "    # initial log-L\n",
    "    logliter = dirmult_logpdf(Xwork, αwork)\n",
    "    \n",
    "    # Use this to efficiently loop through S. Ignores zero entries.\n",
    "    Snnz = Int64.(findmax(Xwork, 2)[1]) - 1\n",
    "\n",
    "    # Max index to loop through for R.\n",
    "    #maxR = length(R) - 1\n",
    "    \n",
    "    littlesum = zeros(T, length(αwork))\n",
    "    S = S'\n",
    "    for iter in 1:maxiters\n",
    "        \n",
    "        for j in 1:length(αwork)\n",
    "            bigsum = 0.0\n",
    "            \n",
    "            for k in 0:Snnz[j]\n",
    "                littlesum[j] += S[k + 1, j] / (αwork[j] + T(k))\n",
    "            end\n",
    "\n",
    "            \n",
    "            for k = 0:maxR\n",
    "                bigsum += R[k + 1] / (sum(αwork) + T(k))\n",
    "            end\n",
    "            αwork[j] = (littlesum[j] / bigsum) * αwork[j]\n",
    "            littlesum[j] = zero(T)\n",
    "        end\n",
    "\n",
    "        loglold  = logliter\n",
    "        logliter = dirmult_logpdf(Xwork, αwork)\n",
    "        # check convergence criterion\n",
    "        if abs(logliter - loglold) < tolfun * (abs(loglold) + 1)\n",
    "            niter = iter\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    αfinal = zeros(eltype(α0), length(α0))\n",
    "    αfinal[rowind] = αwork\n",
    "    ∇final = zeros(eltype(α0), length(α0))\n",
    "    ∇final[rowind] = dirmult_score(Xwork, αwork)\n",
    "    obsinfo = zeros(eltype(α0), length(α0), length(α0))\n",
    "    obsinfo_c, obsinfo_d = dirmult_obsinfo!(obsinfo_dinv, Xwork, αwork)\n",
    "    obsinfo[rowind, rowind] = diagm(obsinfo_d) - obsinfo_c\n",
    "    return logliter, niter, αfinal, ∇final, obsinfo\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton algorithm\n",
      "10×7 DataFrames.DataFrame\n",
      "│ Row │ digit │ n   │ logl_dm  │ logl_mn  │ iters │ LRT │ runtime   │\n",
      "├─────┼───────┼─────┼──────────┼──────────┼───────┼─────┼───────────┤\n",
      "│ 1   │ 0     │ 376 │ -37358.4 │ -39592.2 │ 5     │ 0.0 │ 0.0285187 │\n",
      "│ 2   │ 1     │ 389 │ -42179.2 │ -54039.2 │ 6     │ 0.0 │ 0.0440508 │\n",
      "│ 3   │ 2     │ 380 │ -39985.3 │ -49111.5 │ 6     │ 0.0 │ 0.0416247 │\n",
      "│ 4   │ 3     │ 389 │ -40519.5 │ -47089.1 │ 7     │ 0.0 │ 0.0474771 │\n",
      "│ 5   │ 4     │ 387 │ -43488.8 │ -57344.1 │ 5     │ 0.0 │ 0.0420707 │\n",
      "│ 6   │ 5     │ 376 │ -41191.3 │ -51713.0 │ 7     │ 0.0 │ 0.0498961 │\n",
      "│ 7   │ 6     │ 377 │ -37702.5 │ -42597.3 │ 6     │ 0.0 │ 0.0363099 │\n",
      "│ 8   │ 7     │ 387 │ -40304.0 │ -49473.0 │ 6     │ 0.0 │ 0.0404808 │\n",
      "│ 9   │ 8     │ 380 │ -43130.8 │ -49695.9 │ 6     │ 0.0 │ 0.0384279 │\n",
      "│ 10  │ 9     │ 382 │ -43709.7 │ -54577.8 │ 4     │ 0.0 │ 0.0323011 │\n",
      "\n",
      "MM algorithm\n",
      "10×7 DataFrames.DataFrame\n",
      "│ Row │ digit │ n   │ logl_dm  │ logl_mn  │ iters │ LRT │ runtime   │\n",
      "├─────┼───────┼─────┼──────────┼──────────┼───────┼─────┼───────────┤\n",
      "│ 1   │ 0     │ 376 │ -37359.0 │ -39592.2 │ 95    │ 0.0 │ 0.209583  │\n",
      "│ 2   │ 1     │ 389 │ -42179.4 │ -54039.2 │ 37    │ 0.0 │ 0.0974709 │\n",
      "│ 3   │ 2     │ 380 │ -39985.4 │ -49111.5 │ 6     │ 0.0 │ 0.0296685 │\n",
      "│ 4   │ 3     │ 389 │ -40519.8 │ -47089.1 │ 37    │ 0.0 │ 0.10025   │\n",
      "│ 5   │ 4     │ 387 │ -43488.9 │ -57344.1 │ 21    │ 0.0 │ 0.0699916 │\n",
      "│ 6   │ 5     │ 376 │ -41191.5 │ -51713.0 │ 29    │ 0.0 │ 0.0817985 │\n",
      "│ 7   │ 6     │ 377 │ -37702.8 │ -42597.3 │ 18    │ 0.0 │ 0.051959  │\n",
      "│ 8   │ 7     │ 387 │ -40304.1 │ -49473.0 │ 6     │ 0.0 │ 0.0286776 │\n",
      "│ 9   │ 8     │ 380 │ -43131.2 │ -49695.9 │ 19    │ 0.0 │ 0.0606827 │\n",
      "│ 10  │ 9     │ 382 │ -43709.8 │ -54577.8 │ 31    │ 0.0 │ 0.0866943 │"
     ]
    }
   ],
   "source": [
    "using DataFrames, Distributions\n",
    "\n",
    "ndigit       = zeros(Int, 10)\n",
    "logl_dirmult = zeros(10)\n",
    "iter_dirmult = zeros(Int, 10)\n",
    "time_dirmult = zeros(10)\n",
    "αhat_dirmult = zeros(64, 10)\n",
    "logl_multnom = zeros(10)\n",
    "lrtpval      = zeros(10)\n",
    "print(\"Newton algorithm\\n\")\n",
    "for d in 0:9\n",
    "    # retrieve data for digit d\n",
    "    Xd = traindata[traindata[:, end] .== d, 1:64]'\n",
    "    ndigit[d + 1] = size(Xd, 2)\n",
    "    # fit Dirichlet-multinomial\n",
    "    tic()\n",
    "    logl_dirmult[d + 1], iter_dirmult[d + 1], αhat, = dirmult_newton(Xd)\n",
    "    αhat_dirmult[:, d + 1] = αhat\n",
    "    time_dirmult[d + 1] = toq()\n",
    "    # fit multinomial\n",
    "    p = vec(sum(Xd, 2))\n",
    "    p = p / sum(p)\n",
    "    posind = p .> 0\n",
    "    for j in 1:size(Xd, 2)\n",
    "        logl_multnom[d + 1] += logpdf(Multinomial(Int(sum(Xd[posind, j])), \n",
    "                p[posind]), Xd[posind, j])\n",
    "    end\n",
    "    # compute LRT p-value\n",
    "    lrtpval[d + 1] = ccdf(Chisq(1), logl_dirmult[d + 1] - logl_multnom[d + 1])\n",
    "end\n",
    "result = DataFrame(digit = 0:9, n = ndigit, \n",
    "    logl_dm = logl_dirmult, logl_mn = logl_multnom,\n",
    "    iters = iter_dirmult, LRT = lrtpval, runtime = time_dirmult)\n",
    "print(result)\n",
    "\n",
    "ndigit       = zeros(Int, 10)\n",
    "logl_dirmult = zeros(10)\n",
    "iter_dirmult = zeros(Int, 10)\n",
    "time_dirmult = zeros(10)\n",
    "αhat_dirmult = zeros(64, 10)\n",
    "logl_multnom = zeros(10)\n",
    "lrtpval      = zeros(10)\n",
    "\n",
    "for d in 0:9\n",
    "    # retrieve data for digit d\n",
    "    Xd = traindata[traindata[:, end] .== d, 1:64]'\n",
    "    ndigit[d + 1] = size(Xd, 2)\n",
    "    # fit Dirichlet-multinomial\n",
    "    tic()\n",
    "    logl_dirmult[d + 1], iter_dirmult[d + 1], αhat, = dirmult_mm(Xd)\n",
    "    αhat_dirmult[:, d + 1] = αhat\n",
    "    time_dirmult[d + 1] = toq()\n",
    "    # fit multinomial\n",
    "    p = vec(sum(Xd, 2))\n",
    "    p = p / sum(p)\n",
    "    posind = p .> 0\n",
    "    for j in 1:size(Xd, 2)\n",
    "        logl_multnom[d + 1] += logpdf(Multinomial(Int(sum(Xd[posind, j])), \n",
    "                p[posind]), Xd[posind, j])\n",
    "    end\n",
    "    # compute LRT p-value\n",
    "    lrtpval[d + 1] = ccdf(Chisq(1), logl_dirmult[d + 1] - logl_multnom[d + 1])\n",
    "end\n",
    "print(\"\\n\\nMM algorithm\\n\")\n",
    "result = DataFrame(digit = 0:9, n = ndigit, \n",
    "    logl_dm = logl_dirmult, logl_mn = logl_multnom,\n",
    "    iters = iter_dirmult, LRT = lrtpval, runtime = time_dirmult)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "From the output above I see that the Newton algorithm takes much fewer iterations and faster run-time compared to the MM algorithm. However, a single iteration from the MM algorithm is faster than a single iteration from the Newton algorithm. This is because the Newton algorithm is computing gradients and hessians at each step, whereas the MM algorithm simply updates the $\\alpha$ for each iteration.\n",
    "\n",
    "The Newton algorithm is more efficient since it only takes a few iterations, but the Newton algorithm was much more difficult to derive. For Newton algorithm you have to derive gradients, observed information, and positive definite approximations. For the MM algorithm you only have to derive a minorizing function for the log-likelihood and the maximum of this minorizing function. It's conceptually much simpler.  \n",
    "\n",
    "It makes sense that the Newton algorithm converges faster because it uses more information the the MM algorithm i.e., gradient hessian, and positive definite approximation. You don't get faster convergence for free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "Then term $\\log(\\Gamma(|\\alpha|))$ can be minorized by supporting hyperplane inequality.\n",
    "\n",
    "$\\log(\\Gamma(|\\alpha|)) \\geq \\log(\\Gamma(|\\alpha^{(t)}|)) + \\Psi(|\\alpha^{(t)}|)(|\\alpha| - |\\alpha^{(t)}|)$ where $\\Psi$ is the digamma function. Therefore a minorizing function for $Q(\\alpha\\,|\\,\\alpha^{(t)})$ is $g(\\alpha\\,|\\,\\alpha^{(t)})$, where \n",
    "\n",
    "$$\n",
    "g(\\alpha\\,|\\,\\alpha^{(t)}) = c^{(t)} +n\\Psi(|\\alpha^{(t)}|)|\\alpha| -n\\sum_{j=1}^{d}\\log\\Gamma(\\alpha_j)) + \\sum_{i=1}^{n}\\sum_{j=1}^{d}\\alpha_j\\big(\\Psi(x_{ij}+\\alpha_j^{(t)})-\\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\big)\n",
    "$$\n",
    "\n",
    "Taking derivatives,\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\alpha_k}g(\\alpha\\,|\\,\\alpha^{(t)}) = n\\Psi(|\\alpha^{(t)}|) - n\\Psi(\\alpha_k) + \\sum_{i=1}^{n}\\bigg[\\Psi(x_{ik}+\\alpha_k^{(t)}) - \\Psi(|\\mathbf{x}_i| + |\\alpha^{(t)}|)\\bigg]:= 0\n",
    "$$\n",
    "\n",
    "Solving for $\\alpha_k$, the parameter updates are \n",
    "\n",
    "$$\n",
    "\\alpha_k^{(t+1)} = \\Psi^{-1}\\bigg(\\Psi(|\\alpha^{(t)}|)+\\frac{\\sum_{i=1}^{n}\\Psi(x_{ik}+\\alpha_k^{(t)}) - \\Psi(|\\mathbf{x}_i| + |\\alpha^{(t)}|)}{n}\\bigg)\n",
    "$$\n",
    "\n",
    "This EM-MM hybrid would be easy to implement in a manner similar to problem 5.\n",
    "\n",
    "That's all I have. Thank you for grading this quarter! Very useful class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.1",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
